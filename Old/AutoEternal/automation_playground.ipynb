{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1754"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialization & Imports and such..\n",
    "import json\n",
    "import pyautogui\n",
    "import pytesseract\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from mss import mss\n",
    "from PIL import Image\n",
    "# Uncomment the below if having issues running PyTesseract because it's not found.\n",
    "# pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files (x86)\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Run pip install strsim - This is the pythonn-strinng-similarity library\n",
    "\n",
    "# Import all card names\n",
    "# Pull json back thru json file created above.\n",
    "with open('all_cards.json', 'rb') as f:\n",
    "    all_cards = json.load(f) # json/dict containing total_cards and list of list containing card name and hyperlink for card.\n",
    "# Let's create code to get all card_name's in a list to for our use case...\n",
    "just_names = []\n",
    "for card in all_cards['all_cards']:\n",
    "    just_names.append(card[0])\n",
    "len(just_names) # Should be total number of cards in EternalWarcry db at any time (currently 1754)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This Takes an input of OCR'ed text, and outputs the most similar text to it.\n",
    "'''\n",
    "def find_card(tess_text):\n",
    "    ## TODO: MAKE IT ONLY SEARCH THE SUBSET OF CARDS IN THE DECK WE ARE PLAYING :) \n",
    "    from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
    "    normalized_levenshtein = NormalizedLevenshtein()\n",
    "\n",
    "    prob_card = ''\n",
    "    prob_similarity = float(0.0)\n",
    "\n",
    "    for card in just_names:\n",
    "        current_prob = normalized_levenshtein.similarity(card_name, card)\n",
    "        #print(card + 'Similarity to: ' + card_name + ' is ' + str(current_prob))\n",
    "        if current_prob > prob_similarity:\n",
    "            prob_similarity = current_prob\n",
    "            prob_card = card\n",
    "\n",
    "    print('Winner is: ' + prob_card + '. Original tesseract_text= ' + card_name)\n",
    "    return prob_card\n",
    "\n",
    "'''\n",
    "Methods found online to try different thresholding + binarization methods,\n",
    "and settling on 2/3 being the best. These are use in the third part of process/block.\n",
    "'''\n",
    "\n",
    "def apply_threshold(img, argument):\n",
    "    switcher = {\n",
    "        1: cv2.threshold(cv2.GaussianBlur(img, (9, 9), 0), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1],\n",
    "        2: cv2.threshold(cv2.GaussianBlur(img, (7, 7), 0), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1],\n",
    "        3: cv2.threshold(cv2.GaussianBlur(img, (5, 5), 0), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1],\n",
    "        4: cv2.threshold(cv2.medianBlur(img, 5), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1],\n",
    "        5: cv2.threshold(cv2.medianBlur(img, 3), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1],\n",
    "        6: cv2.adaptiveThreshold(cv2.GaussianBlur(img, (5, 5), 0), 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 2),\n",
    "        7: cv2.adaptiveThreshold(cv2.medianBlur(img, 3), 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 2),\n",
    "    }\n",
    "    return switcher.get(argument, \"Invalid method\")\n",
    "\n",
    "def get_string(img_path, method):\n",
    "    # Read image using opencv\n",
    "    #img = cv2.imread(img_path)# Unncomment if want input to be FILE nnot cv2.im object\n",
    "    img = img_path\n",
    "\n",
    "    # Convert to gray\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply dilation and erosion to remove some noise\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img = cv2.erode(img, kernel, iterations=1)\n",
    "    \n",
    "    # invert threshold\n",
    "    img = cv2.bitwise_not(img)\n",
    "    \n",
    "    # Apply threshold to get image with only black and white\n",
    "    img = apply_threshold(img, method)\n",
    "    \n",
    "    save_path = 'screenshots/post_image_tess.jpg'\n",
    "    cv2.imwrite(save_path, img)\n",
    "\n",
    "    # Recognize text with tesseract for python\n",
    "    result = pytesseract.image_to_string(img, lang=\"eng\")#, config=\"-c tessedit_char_whitelist=0123456789abcdefghijklmnopqrstuvwxyz\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "REQUIREMENT: ETERNAL RUNNING FULLSCREEN/BORDERLESS ON 1920X1080 monitor, and monitor_num=2 if on second monitor.\n",
    "(monitor_num=1 if not). Grabs an image of monitor (1920x1080) every 3 seconds\n",
    "'''\n",
    "# Set which monitor you want (1|2) it to continually capture and save to...\n",
    "monitor_num = 2\n",
    "\n",
    "with mss() as sct:\n",
    "    while 'capturing':\n",
    "        filename = 'screenshots/' + datetime.today().strftime(\"%Y%m%d_%H%M%S.jpg\")\n",
    "        sct.shot(mon=monitor_num, output=filename)\n",
    "        sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winner is: Vara's Favor. Original tesseract_text= 4 Vara's Favor 1\n",
      "\n",
      "_- __\n",
      "' . e\n",
      "\n",
      "\n",
      "\n",
      "Vara's Favor\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Crops image to encompass the region where the CARD_NAMES will be located.\n",
    "Format for cropping = #img[y:y+h, x:x+w]\n",
    "'''\n",
    "# Change this to filename eventually to match\n",
    "screenshot = \"screenshots/7.jpg\"\n",
    "#############################################\n",
    "\n",
    "img = cv2.imread(screenshot)\n",
    "crop_img = img[700:792, 385:1500] # OG FRAME = img[663:788, 385:1500]\n",
    "#cv2.imwrite(screenshot, crop_img)\n",
    "\n",
    "'''\n",
    "Get Card name then compare against imported collection.\n",
    "'''\n",
    "\n",
    "# Use 2 or 3 for deciphering. There is also 'Cronos' lang if running into issue\n",
    "card_name = get_string(crop_img, 2) #input was originally FILE, inputting img from above. (screenshot, 2)\n",
    "\n",
    "if not card_name:# | 'cant find similarity past threshold'\n",
    "    card_name = get_string(crop_img, 3)\n",
    "elif not card_name:\n",
    "    raise ValueError('get_string was unable to to OCR a string. Try all methods 1-7, or re-crop.')\n",
    "#print(card_name)\n",
    "\n",
    "# LAST STEP: Similarity comparisonn\n",
    "true_name = find_card(card_name)\n",
    "print('\\n\\n\\n' + true_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success! Break for today.... Well done Lane :))))))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test stuff below, above we keep./s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mss\n",
    "import mss.tools\n",
    "\n",
    "\n",
    "with mss.mss() as sct:\n",
    "    # Get information of monitor 2\n",
    "    monitor_number = 2\n",
    "    mon = sct.monitors[monitor_number]\n",
    "\n",
    "    # The screen part to capture {X=3077,Y=1034,Width=1006,Height=304}\n",
    "    monitor = {\n",
    "        \"top\": 517,  # 100px from the top mon[\"top\"] + \n",
    "        \"left\": 587,  # 100px from the left mon[\"left\"] + \n",
    "        \"width\": 914,\n",
    "        \"height\": 776,\n",
    "        \"mon\": monitor_number,\n",
    "    }\n",
    "    output = \"sct-mon{mon}_{top}x{left}_{width}x{height}.png\".format(**monitor)\n",
    "\n",
    "    # Grab the data\n",
    "    sct_img = sct.grab(monitor)\n",
    "\n",
    "    # Save to the picture file\n",
    "    mss.tools.to_png(sct_img.rgb, sct_img.size, output=output)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire Sigil\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-974cf85a090c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mth3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#[img, th1, th2, th3]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('screenshots/test5.png',0)\n",
    "img = cv.medianBlur(img,5)\n",
    "#ret,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
    "#th2 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "#            cv.THRESH_BINARY,11,2)\n",
    "th3 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv.THRESH_BINARY,11,2)\n",
    "titles = ['Original Image', 'Global Thresholding (v = 127)',\n",
    "            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\n",
    "images = [th3]#[img, th1, th2, th3]\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "\n",
      "(*~ Cipt\n",
      "\n",
      "k‘ hus' Might\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_string(img_path):\n",
    "    # Read image with opencv\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    # Convert to gray\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "\n",
    "\n",
    "    # Apply dilation and erosion to remove some noise\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img = cv2.erode(img, kernel, iterations=1)\n",
    "\n",
    "    # Write image after removed noise\n",
    "    cv2.imwrite(\"removed_noise.png\", img)\n",
    "\n",
    "    #  Apply threshold to get image with only black and white\n",
    "    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 15, -15)\n",
    "    \n",
    "    # invert threshold\n",
    "    img = cv2.bitwise_not(img)\n",
    "\n",
    "    # Write the image after applying opencv...\n",
    "    cv2.imwrite(\"thresh.png\", img)\n",
    "\n",
    "    # Recognize text with pytesseract\n",
    "    result = pytesseract.image_to_string(Image.open(\"thresh.png\"), lang='eng')#, lang='eng',\n",
    "                        #config='--psm 10 --oem 3 -c tessedit_char_whitelist=0123456789')\n",
    "\n",
    "    return result\n",
    "print(get_string('screenshots/test5.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'screenshots/test.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-299ae13d0ba0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m text = pytesseract.image_to_string(Image.open(\"screenshots/test.png\"), lang='eng',\n\u001b[0m\u001b[0;32m      4\u001b[0m                         config='--oem 3 -c tessedit_char_whitelist=0123456789')\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2476\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2477\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2478\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'screenshots/test.png'"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "text = pytesseract.image_to_string(Image.open(\"screenshots/test.png\"), lang='eng',\n",
    "                        config='--oem 3 -c tessedit_char_whitelist=0123456789')\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*~ Caiphus Might 1k\n"
     ]
    }
   ],
   "source": [
    "# This block takes image and outputs text.\n",
    "# NOTE: youll need to pip Pillow and pytesseract, and then install the windows tesseract along with mapping the path here:\n",
    "# https://github.com/tesseract-ocr/tesseract/wiki/4.0-with-LSTM#400-alpha-for-windows\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files (x86)\\Tesseract-OCR\\tesseract.exe\" # paths correctly\n",
    "\n",
    "try:  \n",
    "    from PIL import Image\n",
    "except ImportError:  \n",
    "    import Image\n",
    "import pytesseract\n",
    "\n",
    "def ocr_core(filename):  \n",
    "    \"\"\"\n",
    "    This function will handle the core OCR processing of images.\n",
    "    \"\"\"\n",
    "    im = Image.open(filename)\n",
    "    text = pytesseract.image_to_string(im, lang=\"eng\")  # We'll use Pillow's Image class to open the image and pytesseract to detect the string in the image\n",
    "    return text\n",
    "\n",
    "print(ocr_core('screenshots/test5.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
